{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "ODsxoCdJPWe0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT2PbGnqPZbu",
        "outputId": "312b1099-927e-4e29-b7c1-733548cd9d92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('bbc_text_cls.csv')"
      ],
      "metadata": {
        "id": "fAWPTk-oPcWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "oOOcKm56Pizd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# populate word2idx\n",
        "# convert documents into sequences of ints / ids / indices\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "tokenized_docs = []\n",
        "for doc in df['text']:\n",
        "  words = word_tokenize(doc.lower())\n",
        "  doc_as_int = []\n",
        "  for word in words:\n",
        "    if word not in word2idx:\n",
        "      word2idx[word] = idx\n",
        "      idx += 1\n",
        "\n",
        "    # save for later\n",
        "    doc_as_int.append(word2idx[word])\n",
        "  tokenized_docs.append(doc_as_int)"
      ],
      "metadata": {
        "id": "gyG2A5IYPjln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mapping\n",
        "# if you do it smarter you can store it as a list\n",
        "idx2word = {v:k for k, v in word2idx.items()}"
      ],
      "metadata": {
        "id": "uADL1mKMPpqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of documents\n",
        "N = len(df['text'])"
      ],
      "metadata": {
        "id": "n5Gfzb4uPxhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of words\n",
        "V = len(word2idx)"
      ],
      "metadata": {
        "id": "KoZX5w3rP1r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate term-frequency matrix\n",
        "# note: could have also used count vectorizer\n",
        "tf = np.zeros((N, V))"
      ],
      "metadata": {
        "id": "gZOlosu4P4wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# populate term-frequency counts\n",
        "for i, doc_as_int in enumerate(tokenized_docs):\n",
        "  for j in doc_as_int:\n",
        "    tf[i, j] += 1"
      ],
      "metadata": {
        "id": "CNcBYTpbP-jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute IDF\n",
        "document_freq = np.sum(tf > 0, axis=0) # document frequency (shape = (V,))\n",
        "idf = np.log(N / document_freq)"
      ],
      "metadata": {
        "id": "mZYCxeHOQBfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute TF-IDF\n",
        "tf_idf = tf * idf"
      ],
      "metadata": {
        "id": "7h2dceSDQFSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)"
      ],
      "metadata": {
        "id": "2MH-oU6iQOGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
        "i = np.random.choice(N)\n",
        "row = df.iloc[i]\n",
        "print(\"Label:\", row['labels'])\n",
        "print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
        "print(\"Top 5 terms:\")\n",
        "\n",
        "scores = tf_idf[i]\n",
        "indices = (-scores).argsort()\n",
        "\n",
        "for j in indices[:5]:\n",
        "  print(idx2word[j])"
      ],
      "metadata": {
        "id": "cPaxCTlVQPFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IT22E4g7QTTD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}